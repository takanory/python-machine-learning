{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第8章 機械学習の適用1 - 感情分析\n",
    "\n",
    "* https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch08/ch08.ipynb\n",
    "* 自然言語処理(Natural Language Processing: NLP)\n",
    "  * 感情(センチメント)分析(sentiment analysis)\n",
    "* 極性(polarity)\n",
    "\n",
    "## 8.1 IMDbの映画レビューデータセットの取得\n",
    "\n",
    "* 意見マイニング(opinion mining)\n",
    "* IMDb(Internet Movie Database)\n",
    "* http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "  * http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "* 以下のコマンドでダウンロード\n",
    "  \n",
    "```sh\n",
    "$ curl -O http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "$ tar xfz aclImdb_v1.tar.gz\n",
    "```\n",
    "\n",
    "* 進行状況を見るために pyprind をインストール\n",
    "\n",
    "```sh\n",
    "$ pip install pyprind\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:01:55 | ETA: 00:01:44 | ETA: 00:01:38 | ETA: 00:01:36 | ETA: 00:01:33 | ETA: 00:01:30 | ETA: 00:01:28 | ETA: 00:01:25 | ETA: 00:01:24 | ETA: 00:01:25 | ETA: 00:01:23 | ETA: 00:01:19 | ETA: 00:01:15 | ETA: 00:01:11 | ETA: 00:01:07 | ETA: 00:01:03 | ETA: 00:00:59 | ETA: 00:00:56 | ETA: 00:00:52 | ETA: 00:00:48 | ETA: 00:00:44 | ETA: 00:00:40 | ETA: 00:00:36 | ETA: 00:00:31 | ETA: 00:00:26 | ETA: 00:00:21 | ETA: 00:00:16 | ETA: 00:00:11 | ETA: 00:00:05 | ETA: 00:00:00 | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:48\n"
     ]
    }
   ],
   "source": [
    "# データを読み込む\n",
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "df = pd.DataFrame()\n",
    "for set in ('test', 'train'):\n",
    "    for label in ('pos', 'neg'):\n",
    "        path = os.path.join('.', 'aclImdb', set, label)\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path, file), encoding='utf-8') as f:\n",
    "                txt = f.read()\n",
    "                df = df.append([[txt, labels[label]]], ignore_index=True)\n",
    "                pbar.update()\n",
    "df.columns = ['review', 'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "# 行の順番をシャッフル\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "# CSV に保存\n",
    "df.to_csv('./movie_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./movie_data.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 BoWモデルの紹介\n",
    "\n",
    "* BoW(Bag-of-Words)\n",
    "\n",
    "1. ドキュメントの集合全体から、たとえば単語という一意なトークン(token)からなる語彙(vocabulary)を作成する\n",
    "2. 各ドキュメントでの各単語の出現回数を含んだ特徴ベクトルを構築する\n",
    "\n",
    "* 疎ベクトル(sparse vector)\n",
    "\n",
    "### 8.2.1 単語を特徴ベクトルに変換する\n",
    "\n",
    "* 生の出現頻度(raw term frequencies): tf(t, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer()\n",
    "docs = np.array([\n",
    "        'The sun is shining',\n",
    "        'The weather is sweet',\n",
    "        'The sun is shining, the weather is sweet, and one and one is two'])\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 1, 'weather': 8, 'sweet': 5, 'shining': 3, 'the': 6, 'one': 2, 'sun': 4, 'and': 0, 'two': 7}\n",
      "[[0 1 0 1 1 0 1 0 0]\n",
      " [0 1 0 0 0 1 1 0 1]\n",
      " [2 3 2 1 1 1 2 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# 語彙の中身を出力\n",
    "print(count.vocabulary_)\n",
    "# 特徴ベクトルを出力\n",
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.2 TF-IDFを使って単語の関連性を評価する\n",
    "\n",
    "* TF-IDF(Term Frequency-Inverse Document Frequency)\n",
    "* TF: 単語の出現頻度\n",
    "* IDF: 逆文書頻度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.    0.43  0.    0.56  0.56  0.    0.43  0.    0.  ]\n",
      " [ 0.    0.43  0.    0.    0.    0.56  0.43  0.    0.56]\n",
      " [ 0.5   0.45  0.5   0.19  0.19  0.19  0.3   0.25  0.19]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
